{"cells":[{"cell_type":"markdown","metadata":{"id":"FcD3xJr8qyOM"},"source":["## Movie Reviews Sentiment Classification\n","\n","Classify movie reviews into positive, negative and neutral classes. We are using IMDB 50k movie reviews data. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esPM3yLcx3DE"},"outputs":[],"source":["# !pip install contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22569,"status":"ok","timestamp":1725090795679,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"f_rwW826gXoo","outputId":"f4fdf4a9-93f5-4aec-d3b6-70d911a5cf8a"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')\n","import re\n","import contractions\n","import unicodedata\n","import html\n","\n","from nltk.stem import WordNetLemmatizer\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from nltk.tokenize import word_tokenize\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"markdown","metadata":{},"source":["Import data from kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8331X0Qg0Ol"},"outputs":[],"source":["# !mkdir -p ~/.kaggle\n","# !mv kaggle.json ~/.kaggle/\n","# !chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2409,"status":"ok","timestamp":1724591086650,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"qpfXDbRmow1m","outputId":"2fbe3724-f596-4285-c2bb-ef3bf6b39ccb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","License(s): other\n","Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n"," 66% 17.0M/25.7M [00:00<00:00, 71.7MB/s]\n","100% 25.7M/25.7M [00:00<00:00, 88.5MB/s]\n"]}],"source":["# !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1789,"status":"ok","timestamp":1724591164830,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"TSEj7aXDo4_V","outputId":"6c71dac4-d3e1-42c8-9d00-f76167ac748e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  imdb-dataset-of-50k-movie-reviews.zip\n","  inflating: /content/drive/MyDrive/Sentiment_analysis/IMDB Dataset.csv  \n"]}],"source":["# !unzip imdb-dataset-of-50k-movie-reviews.zip -d /content/drive/MyDrive/Sentiment_analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"we46EghzpEoJ"},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/Sentiment_analysis/IMDB Dataset.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1725090855271,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"GwCRraq2pVTc","outputId":"2e747a52-6dc0-4ae3-f602-2800bae1de69"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>sentiment</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>positive</th>\n","      <td>25000</td>\n","    </tr>\n","    <tr>\n","      <th>negative</th>\n","      <td>25000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"],"text/plain":["sentiment\n","positive    25000\n","negative    25000\n","Name: count, dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df['sentiment'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHFU7PuWpoqe"},"outputs":[],"source":["def preprocess(text):\n","\n","    text = text.lower() # lowercase\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # remove links\n","    text = re.sub(r'<br />', ' ', text)\n","    text = re.sub(r'/', ' ', text) # convert / to space\n","    text = contractions.fix(text) # convert contractions to single words\n","    text = re.sub(r'[^a-z\\s]', ' ', text) # remove punctuations\n","    text = re.sub(r'\\bs\\b', '', text) # remove extra spaces\n","\n","    return ' '.join(text.split())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIJXQ00i0UOA"},"outputs":[],"source":["df['preprocess_review'] = df['review'].apply(preprocess)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-S0SloL2eLu"},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cwx7jxWyTS0"},"outputs":[],"source":["# Replace repeated characters with two occurrences\n","def normalize_stretched_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1\\1', word)  \n","\n","def lemmatize_and_normalize_text(text):\n","    tokens = word_tokenize(text.lower())\n","    normalized_tokens = [normalize_stretched_word(token) for token in tokens]\n","    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in normalized_tokens]\n","    return ' '.join(lemmatized_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uh-DLsXq2lHt"},"outputs":[],"source":["df['preprocess_review'] = df['preprocess_review'].apply(lemmatize_and_normalize_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AnvNQkAX1_gF"},"outputs":[],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['preprocess_review'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNyqWPP36AMu"},"outputs":[],"source":["word_index = tokenizer.word_index\n","vocab_size = len(word_index) + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1725091044410,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"PLxtAVU86KV7","outputId":"cf52a767-7007-478c-f5e3-bce4931dd404"},"outputs":[{"data":{"text/plain":["89202"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217724,"status":"ok","timestamp":1725091281288,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"llFNnw3h6MaR","outputId":"fbde5ef1-04eb-49fa-d640-1e3c2c317677"},"outputs":[{"name":"stdout","output_type":"stream","text":["Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: '.'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'Killerseats.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'mylot.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n","Skipping line due to error: could not convert string to float: 'Amazon.com'\n","Skipping line due to error: could not convert string to float: 'name@domain.com'\n"]}],"source":["# Use glove embeddings from the file\n","\n","def load_glove_embeddings(file_path):\n","    embeddings_index = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            try:\n","                coefs = np.asarray(values[1:], dtype='float32')\n","                embeddings_index[word] = coefs\n","            except ValueError as e:\n","                print(f\"Skipping line due to error: {e}\")\n","                continue\n","    return embeddings_index\n","\n","glove_file_path = '/path/to/glove.840B.300d.txt'  # Replace with your actual path to your GloVe file\n","embeddings_index = load_glove_embeddings(glove_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSf_rtIo6hid"},"outputs":[],"source":["# create a embedding matrix for our vocabulary\n","\n","embedding_dim = 300 \n","glove_embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n","\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # Words not found in embedding index will be all-zeros\n","        glove_embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUcKQDOW7kza"},"outputs":[],"source":["df['sequences'] = tokenizer.texts_to_sequences(df['preprocess_review'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":481,"status":"ok","timestamp":1725091303294,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"s3auu-OA71Ca","outputId":"b2f3cd15-4399-4603-c421-442c78087e10"},"outputs":[{"data":{"text/plain":["21525"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["empty_rows = np.all(glove_embedding_matrix == 0, axis=1) #no. of tokens with no word embedding\n","np.sum(empty_rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tnT_Xycf8COt"},"outputs":[],"source":["df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1725091305962,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"EZ-sjZDG8q7j","outputId":"0acd7f26-d202-4a29-b9e7-4b1a45d2644c"},"outputs":[{"name":"stdout","output_type":"stream","text":["2474\n","175.0\n"]}],"source":["print(df['sequences'].apply(len).max())\n","print(df['sequences'].apply(len).median())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pm8tEJrP9QVY"},"outputs":[],"source":["import pickle\n","\n","with open('/content/drive/MyDrive/Sentiment_analysis/data.pkl', 'wb') as f:\n","    pickle.dump({\n","        'data': df,\n","        'tokenizer': tokenizer,\n","        'embedding_matrix': glove_embedding_matrix\n","    }, f)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNb28wg6dfRRbvo7+x4tQK1","mount_file_id":"1R7mlRyNLhmeZMCjinHurLDwGfwIbKcq3","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
