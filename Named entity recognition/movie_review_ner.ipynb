{"cells":[{"cell_type":"markdown","metadata":{},"source":["Named Entity Recognition (NER) is a task in Natural Language Processing (NLP) that involves identifying and classifying important entities in a text.\n","\n","NER models are designed to take a sequence of text (such as sentences or paragraphs) and label each word or token with a tag representing its entity type. In this partiular project, we are working on a movies dataset with tags such as Actor, director, genre, character, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1720,"status":"ok","timestamp":1726147801467,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"FsqEqGfB3JA7","outputId":"1f7d1ed9-b1de-4bbd-e4f1-064f8616ca9d"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import sent_tokenize\n","from nltk.corpus import wordnet\n","from collections import Counter\n","\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n","from keras.preprocessing.sequence import pad_sequences\n","nltk.download('wordnet')\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{},"source":["We are using two different datasets for our model. Both datasets have the same tags with different names. So in order to merge the datasets and use it in the model, we must generalize the tag names. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDFv5_pM675v"},"outputs":[],"source":["replace_dict = {\n","    'Actor': \"ACTOR\",\n","    'Character_Name': \"CHARACTER\",\n","    'Director': \"DIRECTOR\",\n","    'Genre': \"GENRE\",\n","    'Plot': \"PLOT\",\n","    'Opinion': \"REVIEW\",\n","    'Soundtrack': \"SONG\",\n","    'Year': \"YEAR\",\n","    'Award': \"AWARD\",\n","    'Relationship': \"RELATIONSHIP\",\n","    'Origin': \"ORIGIN\",\n","    'Quote': \"QUOTE\"\n","}\n","\n","# we will only use a few tags for this project and remove all else. You can use these tags if you want.\n","remove_tags = ['B-TITLE','I-TITLE','B-PLOT','I-PLOT', 'B-ORIGIN', 'I-ORIGIN','B-RELATIONSHIP', 'I-RELATIONSHIP', 'I-TRAILER', 'B-TRAILER', 'I-RATINGS_AVERAGE', 'B-RATINGS_AVERAGE', 'I-SONG', 'B-SONG', 'I-REVIEW', 'B-REVIEW', 'B-QUOTE', 'I-QUOTE']\n","\n","def replace_tags(label):\n","    label_ = label.split(\"-\")\n","    new_label = \"\"\n","\n","    if len(label_) > 1:\n","        if label_[1] in replace_dict:\n","            new_label = f\"{label_[0]}-{replace_dict[label_[1]]}\"\n","        else:\n","            new_label = label\n","    else:\n","        new_label = label\n","\n","    if new_label in remove_tags:\n","        return \"O\"\n","\n","    return new_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AH_1zWuA7W0h"},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()\n","\n","# Preprocess words by removing some contractions and splitting into individual tokens. \n","# As our datasets are mostly clean, there is no need to preprocess a lot.\n","\n","def preprocess_words(text):\n","    # text = re.sub(r'\\bs\\b', 'is', text)\n","    text = re.sub(r'\\bm\\b', 'am', text)\n","    text = re.sub(r'\\bcan t\\b', 'can not', text)\n","\n","    # text = [lemmatizer.lemmatize(word) for word in text.split()]\n","    return text.split()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLZFvPeX3L2Z"},"outputs":[],"source":["# load the data from the text file and apply preprocessing\n","def load_data(path):\n","    sentence = []\n","    sentences = []\n","\n","    with open(path, 'r') as file:\n","\n","        for line in file:\n","            if line.strip():\n","                tag, word = line.split()\n","                sentence.append((word, replace_tags(tag)))\n","            else:\n","                if sentence:\n","\n","                  word_sentence = ' '.join([word for word, tag in sentence])\n","                  tags = [tag for word, tag in sentence]\n","                  sentence = preprocess_words(word_sentence)\n","                  sentences.append((sentence, tags))\n","                  sentence = []\n","\n","    return sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WOkOPZ5Y4reb"},"outputs":[],"source":["train = load_data('/content/drive/MyDrive/NER/Data/train.txt')\n","engtrain = load_data('/content/drive/MyDrive/NER/Data/engtrain.bio.txt')\n","test = load_data('/content/drive/MyDrive/NER/Data/test.txt')\n","engtest = load_data('/content/drive/MyDrive/NER/Data/engtest.bio.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERmKLhmr5jJu"},"outputs":[],"source":["# Merge the two datasets\n","train = train + engtrain\n","test = test + engtest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cA5q6Uz2YkY1"},"outputs":[],"source":["# seperate the sentences and tags from the data tuples\n","def seperate_data(data):\n","  sentences = []\n","  tags = []\n","\n","  for i in data:\n","    assert len(i[0]) == len(i[1])\n","    sentences.append(i[0])\n","    tags.append(i[1])\n","\n","  return sentences, tags\n","\n","train_sentences, train_tags = seperate_data(train)\n","test_sentences, test_tags = seperate_data(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"quDKLGjvZqyp"},"outputs":[],"source":["# Function to map tokens or tags to their respective index (token2idx, tag2idx) and reverse (idx2token, idx2tag)\n","# adding a 'PAD' token for padding and 'UNK' token for unknown tokens \n","\n","def get_dict_map(data, token_or_tag):\n","    tok2idx = {}\n","    idx2tok = {}\n","    if token_or_tag == 'token':\n","        vocab = list(set([word for sentence in data for word in sentence]))\n","        vocab = ['PAD', 'UNK'] + vocab\n","    else:\n","        vocab = list(set([tag for sentence in data for tag in sentence]))\n","        vocab = ['PAD'] + vocab\n","\n","    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n","    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n","\n","    return tok2idx, idx2tok\n","\n","token2idx, idx2token = get_dict_map(train_sentences, 'token')\n","tag2idx, idx2tag = get_dict_map(train_tags, 'tag')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1726148347878,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"T0_D25Qxie5K","outputId":"78650079-8cb7-45c8-e6cb-05b00a231fd2"},"outputs":[{"data":{"text/plain":["{'PAD': 0,\n"," 'I-GENRE': 1,\n"," 'O': 2,\n"," 'B-YEAR': 3,\n"," 'I-RATING': 4,\n"," 'B-CHARACTER': 5,\n"," 'B-DIRECTOR': 6,\n"," 'B-GENRE': 7,\n"," 'I-AWARD': 8,\n"," 'I-YEAR': 9,\n"," 'B-AWARD': 10,\n"," 'B-RATING': 11,\n"," 'I-DIRECTOR': 12,\n"," 'I-ACTOR': 13,\n"," 'B-ACTOR': 14,\n"," 'I-CHARACTER': 15}"]},"execution_count":129,"metadata":{},"output_type":"execute_result"}],"source":["tag2idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrFCeEVHZatW"},"outputs":[],"source":["def prepare_data(sentences, tags, token2idx, tag2idx, max_len):\n","\n","    X = [[token2idx.get(token, token2idx['UNK']) for token in sentence] for sentence in sentences]\n","    y = [[tag2idx[tag] for tag in sentence] for sentence in tags]\n","    X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=token2idx[\"PAD\"])\n","    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n","\n","    return X, y\n","\n","X_train, y_train = prepare_data(train_sentences, train_tags, token2idx, tag2idx, 50)\n","X_test, y_test = prepare_data(test_sentences, test_tags, token2idx, tag2idx, 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUr2Eb9seCMl"},"outputs":[],"source":["# One-hot encoding\n","y_tr = [to_categorical(i, num_classes=len(tag2idx)) for i in y_train]\n","y_te = [to_categorical(i, num_classes=len(tag2idx)) for i in y_test]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCbEd_4teUbv"},"outputs":[],"source":["# Create an embedding matrix where each token is mapped to its corresponding Word2Vec vector\n","from gensim.models import KeyedVectors\n","word2vec = KeyedVectors.load('/path/to/word2vec-google-news-300.model')\n","\n","vocab_size = len(token2idx)\n","embedding_dim = 300\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","for word, i in token2idx.items():\n","    if word in word2vec:\n","        embedding_matrix[i] = word2vec.get_vector(word)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":875,"status":"ok","timestamp":1726148382305,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"fQWm9TEddELH","outputId":"a8b30ba2-61ce-4fab-b010-061fe254c221"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]}],"source":["# Model architecture\n","model = Sequential()\n","\n","model.add(Embedding(input_dim=vocab_size,\n","                    output_dim=embedding_dim,\n","                    weights=[embedding_matrix],\n","                    input_length=50,\n","                    trainable=False))\n","\n","model.add(Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))) # As NER is a token-level task, we need to keep 'return-sequence' true to get a sequence of output instead of just one final output/\n","model.add(LSTM(units=50, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n","model.add(TimeDistributed(Dense(len(tag2idx), activation=\"softmax\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":773143,"status":"ok","timestamp":1726149155442,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"ox1dm2oxi3PK","outputId":"203d34dd-20ee-4cd5-a037-a9447b205c34"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 193ms/step - accuracy: 0.8857 - loss: 0.5950 - precision: 0.9410 - recall: 0.7707 - val_accuracy: 0.9431 - val_loss: 0.2219 - val_precision: 0.9639 - val_recall: 0.9371\n","Epoch 2/10\n","\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 184ms/step - accuracy: 0.9482 - loss: 0.2045 - precision: 0.9704 - recall: 0.9363 - val_accuracy: 0.9613 - val_loss: 0.1320 - val_precision: 0.9823 - val_recall: 0.9471\n","Epoch 3/10\n","\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 209ms/step - accuracy: 0.9630 - loss: 0.1318 - precision: 0.9815 - recall: 0.9483 - val_accuracy: 0.9741 - val_loss: 0.0924 - val_precision: 0.9838 - val_recall: 0.9647\n","Epoch 4/10\n","\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 183ms/step - accuracy: 0.9737 - loss: 0.0975 - precision: 0.9838 - recall: 0.9637 - val_accuracy: 0.9801 - val_loss: 0.0721 - val_precision: 0.9852 - val_recall: 0.9756\n","Epoch 5/10\n","\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 183ms/step - accuracy: 0.9782 - loss: 0.0802 - precision: 0.9849 - recall: 0.9723 - val_accuracy: 0.9826 - val_loss: 0.0616 - val_precision: 0.9861 - val_recall: 0.9797\n","Epoch 6/10\n","\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 190ms/step - accuracy: 0.9810 - loss: 0.0696 - precision: 0.9859 - recall: 0.9770 - val_accuracy: 0.9840 - val_loss: 0.0561 - val_precision: 0.9870 - val_recall: 0.9814\n","Epoch 7/10\n","\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 193ms/step - accuracy: 0.9827 - loss: 0.0631 - precision: 0.9868 - recall: 0.9790 - val_accuracy: 0.9850 - val_loss: 0.0516 - val_precision: 0.9876 - val_recall: 0.9830\n","Epoch 8/10\n","\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 190ms/step - accuracy: 0.9841 - loss: 0.0573 - precision: 0.9876 - recall: 0.9811 - val_accuracy: 0.9859 - val_loss: 0.0488 - val_precision: 0.9886 - val_recall: 0.9838\n","Epoch 9/10\n","\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 191ms/step - accuracy: 0.9849 - loss: 0.0548 - precision: 0.9881 - recall: 0.9823 - val_accuracy: 0.9866 - val_loss: 0.0472 - val_precision: 0.9889 - val_recall: 0.9848\n","Epoch 10/10\n","\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 190ms/step - accuracy: 0.9857 - loss: 0.0519 - precision: 0.9885 - recall: 0.9831 - val_accuracy: 0.9869 - val_loss: 0.0460 - val_precision: 0.9890 - val_recall: 0.9852\n"]}],"source":["model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"precision\", \"recall\"])\n","\n","# Train the model\n","history = model.fit(\n","    X_train,\n","    np.array(y_tr),\n","    batch_size=64,\n","    epochs=10,\n","    validation_data=(X_test, np.array(y_te)),\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10509,"status":"ok","timestamp":1726149219261,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"LEPWLwPQdooX","outputId":"b58e454e-4acb-42d4-9527-633f284d505d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 55ms/step\n","Precision: 0.7472859566359298\n","Recall: 0.8345525883025926\n","F1-Score: 0.7699618790547424\n","\n","Detailed Report:\n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00    156120\n","           1       0.73      0.82      0.77       680\n","           2       0.99      0.97      0.98     51709\n","           3       0.89      0.90      0.90      1355\n","           4       0.87      0.94      0.90       209\n","           5       0.23      0.52      0.31       161\n","           6       0.78      0.88      0.83       780\n","           7       0.89      0.86      0.87      1990\n","           8       0.69      0.59      0.64       174\n","           9       0.94      0.93      0.94       663\n","          10       0.14      0.75      0.23        12\n","          11       0.96      0.96      0.96       503\n","          12       0.80      0.90      0.85       806\n","          13       0.89      0.90      0.89      2377\n","          14       0.90      0.88      0.89      2115\n","          15       0.26      0.54      0.35       146\n","\n","    accuracy                           0.99    219800\n","   macro avg       0.75      0.83      0.77    219800\n","weighted avg       0.99      0.99      0.99    219800\n","\n"]}],"source":["from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n","\n","y_preds = model.predict(X_test)\n","\n","y_test_flat = np.array(y_test).flatten()\n","y_preds_flat = np.argmax(y_preds, axis=-1).flatten()\n","\n","precision = precision_score(y_preds_flat, y_test_flat, average='macro')\n","recall = recall_score(y_preds_flat, y_test_flat, average='macro', zero_division=0)\n","f1 = f1_score(y_preds_flat, y_test_flat, average='macro', zero_division=0)\n","\n","# Print classification report for detailed results\n","report = classification_report(y_preds_flat, y_test_flat, zero_division=0)\n","\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-Score:\", f1)\n","print(\"\\nDetailed Report:\\n\", report)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIoTxYhAhAav"},"outputs":[],"source":["def predict(text):\n","  sentences = sent_tokenize(text)\n","  all_results = []\n","\n","  for sentence in sentences:\n","\n","      sentence = sentence.lower()\n","      sentence = re.sub(r'[^a-zA-Z0-9\\s]', ' ', sentence)\n","      sentence = preprocess_words(sentence)\n","      sentence = [token2idx.get(token, token2idx['UNK']) for token in sentence]\n","      sentence = pad_sequences(maxlen=50, sequences=[sentence], padding=\"post\", value=token2idx[\"PAD\"])\n","      y_pred = model.predict(sentence.reshape(1, sentence.shape[1]))\n","      y_pred = [idx2tag[id] for id in np.argmax(y_pred, axis=-1)[0]]\n","\n","      result = []\n","      for token, tag in zip(sentence[0], y_pred):\n","          if token == token2idx[\"PAD\"]:\n","              break\n","          result.append((idx2token[token], tag))\n","      all_results.append(result)\n","\n","  return all_results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":602,"status":"ok","timestamp":1726149253166,"user":{"displayName":"Abhishek maher","userId":"17060706371112675718"},"user_tz":-330},"id":"D4LKKl-bfiCp","outputId":"beefa9e3-a388-47a8-d20c-7679a8b3f5e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n"]},{"data":{"text/plain":["[[('leonardo', 'B-ACTOR'),\n","  ('dicaprio', 'I-ACTOR'),\n","  ('s', 'O'),\n","  ('performance', 'O'),\n","  ('in', 'O'),\n","  ('inception', 'O'),\n","  ('was', 'O'),\n","  ('UNK', 'O')],\n"," [('christopher', 'B-ACTOR'),\n","  ('nolan', 'I-DIRECTOR'),\n","  ('s', 'O'),\n","  ('direction', 'O'),\n","  ('brought', 'O'),\n","  ('out', 'O'),\n","  ('the', 'O'),\n","  ('best', 'O'),\n","  ('in', 'O'),\n","  ('the', 'O'),\n","  ('cast', 'O')],\n"," [('the', 'O'),\n","  ('movie', 'O'),\n","  ('set', 'O'),\n","  ('in', 'O'),\n","  ('paris', 'O'),\n","  ('and', 'O'),\n","  ('other', 'O'),\n","  ('global', 'O'),\n","  ('locations', 'O'),\n","  ('was', 'O'),\n","  ('a', 'O'),\n","  ('masterpiece', 'O'),\n","  ('of', 'O'),\n","  ('visual', 'O'),\n","  ('effects', 'O'),\n","  ('and', 'O'),\n","  ('storytelling', 'O')],\n"," [('i', 'O'),\n","  ('can', 'O'),\n","  ('not', 'O'),\n","  ('wait', 'O'),\n","  ('to', 'O'),\n","  ('see', 'O'),\n","  ('what', 'O'),\n","  ('nolan', 'B-ACTOR'),\n","  ('does', 'O'),\n","  ('next', 'O')]]"]},"execution_count":140,"metadata":{},"output_type":"execute_result"}],"source":["text = \"Leonardo DiCaprio's performance in Inception was mesmerizing. Christopher Nolan's direction brought out the best in the cast. The movie, set in Paris and other global locations, was a masterpiece of visual effects and storytelling. I can’t wait to see what Nolan does next!\"\n","predict(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9r3WivXjfTn"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOhegt+W027WRk5hPqdkRgt","mount_file_id":"15FxbLgqHzzVgBXODSiLMrh8uWkVeBo7j","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
